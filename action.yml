name: 'ReviewPal'
description: 'AI-powered code review for any language using Gemini'
author: 'Han Kim'

branding:
  icon: 'eye'
  color: 'purple'

inputs:
  gemini_api_key:
    description: 'Google Gemini API key (REQUIRED)'
    required: true
  github_token:
    description: 'GitHub token for posting comments (defaults to GITHUB_TOKEN)'
    required: false
    default: ${{ github.token }}
  model:
    description: 'Gemini model to use'
    required: false
    default: 'gemini-2.5-pro'
  max_api_calls:
    description: 'Maximum API calls (1 triage + N deep reviews + 3 adversarial)'
    required: false
    default: '10'
  comment_on_pr:
    description: 'Post analysis as PR comment'
    required: false
    default: 'true'

outputs:
  total_files:
    description: 'Number of files analyzed'
  total_hunks:
    description: 'Number of hunks reviewed'
  verdict:
    description: 'Review verdict: BLOCK, WARN, or CLEAR'
  analysis_markdown:
    description: 'Full analysis in markdown format'

runs:
  using: 'composite'
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'

    - name: Install dependencies
      shell: bash
      run: |
        cd ${{ github.action_path }}
        npm ci --production

    - name: Build
      shell: bash
      run: |
        cd ${{ github.action_path }}
        npm run build

    - name: Get PR diff
      shell: bash
      id: diff
      run: |
        git fetch origin ${{ github.base_ref }} --depth=1
        git diff origin/${{ github.base_ref }}...HEAD > /tmp/pr.diff
        echo "diff_file=/tmp/pr.diff" >> $GITHUB_OUTPUT
        echo "lines=$(wc -l < /tmp/pr.diff)" >> $GITHUB_OUTPUT

    - name: Skip if no diff
      if: steps.diff.outputs.lines == '0'
      shell: bash
      run: |
        echo "No changes detected, skipping analysis"
        echo "total_files=0" >> $GITHUB_OUTPUT
        echo "total_hunks=0" >> $GITHUB_OUTPUT
        exit 0

    - name: Run analysis
      shell: bash
      id: analyze
      env:
        GEMINI_API_KEY: ${{ inputs.gemini_api_key }}
        GITHUB_TOKEN: ${{ inputs.github_token }}
        GH_TOKEN: ${{ inputs.github_token }}
        GITHUB_REPOSITORY: ${{ github.repository }}
        GITHUB_WORKSPACE: ${{ github.workspace }}
        PR_NUMBER: ${{ github.event.pull_request.number }}
        HEAD_SHA: ${{ github.event.pull_request.head.sha }}
      run: |
        cd ${{ github.action_path }}

        # Single analysis run as JSON (saves ~50% API cost vs running twice)
        node dist/index.js ${{ steps.diff.outputs.diff_file }} \
          --format json \
          --quiet \
          --repo-root "$GITHUB_WORKSPACE" \
          --model ${{ inputs.model }} \
          --max-api-calls ${{ inputs.max_api_calls }} > /tmp/analysis.json 2>&1 || echo '{"files":[],"totalHunks":0}' > /tmp/analysis.json

        # Extract key metrics
        echo "total_files=$(jq '.files | length // 0' /tmp/analysis.json)" >> $GITHUB_OUTPUT
        echo "total_hunks=$(jq '.totalHunks // 0' /tmp/analysis.json)" >> $GITHUB_OUTPUT
        echo "verdict=$(jq -r '.verdict.verdict // "CLEAR"' /tmp/analysis.json)" >> $GITHUB_OUTPUT

    - name: Post review and summary
      if: inputs.comment_on_pr == 'true' && github.event_name == 'pull_request' && steps.analyze.outputs.total_hunks != '0'
      id: post
      uses: actions/github-script@v7
      with:
        github-token: ${{ inputs.github_token }}
        script: |
          const fs = require('fs');
          const analysisJson = JSON.parse(fs.readFileSync('/tmp/analysis.json', 'utf8'));

          const EMOJI_MAP = {
            security: 'ğŸ”’',
            crash: 'ğŸ’¥',
            'data-loss': 'ğŸ—‘ï¸',
            performance: 'ğŸŒ',
            regression: 'ğŸ”„',
            logic: 'ğŸ§©'
          };

          const VERDICT_EMOJI = {
            BLOCK: 'ğŸ”´',
            WARN: 'ğŸŸ¡',
            CLEAR: 'ğŸŸ¢'
          };

          const FINDING_MARKER = '<!-- reviewpal-finding -->';
          const SUMMARY_MARKER = '<!-- reviewpal -->';

          // â”€â”€ Step 1: Parse diff to find valid line numbers and .not() usage â”€â”€

          const diffContent = fs.readFileSync('/tmp/pr.diff', 'utf8');
          const validLines = {};
          const dotNotLines = [];
          let currentFile = null;
          let newLineNum = 0;
          for (const line of diffContent.split('\n')) {
            const fileMatch = line.match(/^\+\+\+ b\/(.+)/);
            if (fileMatch) {
              currentFile = fileMatch[1];
              if (!validLines[currentFile]) validLines[currentFile] = new Set();
              continue;
            }
            const hunkMatch = line.match(/^@@ -\d+(?:,\d+)? \+(\d+)(?:,\d+)? @@/);
            if (hunkMatch) {
              newLineNum = parseInt(hunkMatch[1], 10);
              continue;
            }
            if (currentFile && !line.startsWith('---') && !line.startsWith('diff ')) {
              if (line.startsWith('+') || line.startsWith(' ')) {
                validLines[currentFile].add(newLineNum);
                if (line.startsWith('+') && line.match(/\.not\(\)/)) {
                  dotNotLines.push({ file: currentFile, line: newLineNum });
                }
                newLineNum++;
              } else if (line.startsWith('-')) {
                // Deleted lines don't increment new file line counter
              } else {
                newLineNum++;
              }
            }
          }

          function closestValidLine(filePath, targetLine) {
            const lines = validLines[filePath];
            if (!lines || lines.size === 0) return null;
            const sorted = [...lines].sort((a, b) => a - b);
            let best = sorted[0];
            for (const l of sorted) {
              if (Math.abs(l - targetLine) < Math.abs(best - targetLine)) best = l;
            }
            return best;
          }

          // â”€â”€ Step 2: Delete old ReviewPal inline comments â”€â”€

          let page = 1;
          let oldComments = [];
          while (true) {
            const { data: batch } = await github.rest.pulls.listReviewComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number,
              per_page: 100,
              page
            });
            if (batch.length === 0) break;
            oldComments.push(...batch);
            if (batch.length < 100) break;
            page++;
          }

          // Match both new format (marker) and old format (emoji + bold type)
          const REVIEWPAL_EMOJI_PATTERN = /^(<!-- reviewpal-finding -->|ğŸ”’ \*\*|ğŸ’¥ \*\*|ğŸ—‘ï¸ \*\*|ğŸŒ \*\*|ğŸ”„ \*\*|ğŸ§© \*\*|âš ï¸ \*\*)/;
          const staleComments = oldComments.filter(
            c => c.body && (c.body.includes(FINDING_MARKER) || c.body.trim() === 'ğŸ¤Œ' || REVIEWPAL_EMOJI_PATTERN.test(c.body.trim()))
          );

          if (staleComments.length > 0) {
            console.log(`Cleaning up ${staleComments.length} old ReviewPal comment(s)`);
            for (const old of staleComments) {
              try {
                await github.rest.pulls.deleteReviewComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: old.id
                });
              } catch (e) {
                console.log(`Failed to delete comment ${old.id}: ${e.message}`);
              }
            }
          }

          // â”€â”€ Step 3: Collect all findings â”€â”€

          const allIssues = [];
          for (const file of analysisJson.files || []) {
            for (const hunk of file.hunks || []) {
              if (hunk.aiReview && hunk.aiReview.critical) {
                for (const issue of hunk.aiReview.critical) {
                  allIssues.push({
                    file: file.filename,
                    line: issue.line,
                    type: issue.type,
                    friendlySuggestion: issue.friendlySuggestion || issue.issue,
                    source: 'deep-review'
                  });
                }
              }
            }
          }

          for (const finding of analysisJson.adversarialFindings || []) {
            allIssues.push({
              file: finding.filename,
              line: finding.line,
              type: finding.type,
              friendlySuggestion: finding.friendlySuggestion || finding.issue,
              source: 'adversarial',
              persona: finding.persona
            });
          }

          const verdict = analysisJson.verdict || null;
          const prSummary = (analysisJson.triage && analysisJson.triage.prSummary) || '';
          const headSha = context.payload.pull_request.head.sha;

          // â”€â”€ Step 4: Build review comments (batched) â”€â”€

          const reviewComments = [];
          const failedIssues = [];

          for (const item of allIssues) {
            const diffLine = closestValidLine(item.file, item.line);
            if (!diffLine) {
              console.log(`No valid diff lines for ${item.file}, will include in summary`);
              failedIssues.push(item);
              continue;
            }
            if (Math.abs(diffLine - item.line) > 0) {
              console.log(`Snapped ${item.file}:${item.line} -> ${diffLine} (nearest diff line)`);
            }

            let body = `${FINDING_MARKER}\n${EMOJI_MAP[item.type] || 'âš ï¸'} **${item.type.toUpperCase()}**: ${item.friendlySuggestion}`;
            if (item.source === 'adversarial' && item.persona) {
              body += `\n\n_Found by ${item.persona}_`;
            }

            reviewComments.push({
              path: item.file,
              line: diffLine,
              side: 'RIGHT',
              body
            });
          }

          // Add chef's kiss comments for .not() usage
          for (const dot of dotNotLines) {
            const diffLine = closestValidLine(dot.file, dot.line);
            if (diffLine) {
              reviewComments.push({
                path: dot.file,
                line: diffLine,
                side: 'RIGHT',
                body: `${FINDING_MARKER}\nğŸ¤Œ`
              });
            }
          }

          // Submit as a single batched review (one notification instead of N)
          if (reviewComments.length > 0) {
            try {
              await github.rest.pulls.createReview({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.issue.number,
                commit_id: headSha,
                event: 'COMMENT',
                comments: reviewComments
              });
              console.log(`Submitted review with ${reviewComments.length} inline comment(s)`);
            } catch (e) {
              console.log(`Batched review failed: ${e.message}`);
              // All issues fall back to summary
              failedIssues.push(...allIssues);
            }
          }

          // â”€â”€ Step 5: Build and post summary comment â”€â”€

          let summary = '';

          if (verdict) {
            const vEmoji = VERDICT_EMOJI[verdict.verdict] || 'âšª';
            summary += `## ${vEmoji} Verdict: ${verdict.verdict}\n\n`;
            summary += `> ${verdict.reason}\n\n---\n\n`;
          }

          if (prSummary) {
            summary += `## ğŸ“‹ What is this PR about\n\n${prSummary}\n\n---\n\n`;
          }

          if (allIssues.length === 0 && !verdict) {
            summary += `âœ… **No critical issues found**`;
          } else if (allIssues.length > 0) {
            // Group issues by type for the summary table
            const issuesByType = {};
            for (const item of allIssues) {
              if (!issuesByType[item.type]) issuesByType[item.type] = [];
              issuesByType[item.type].push(item);
            }

            const severityOrder = ['security', 'crash', 'data-loss', 'performance', 'regression', 'logic'];
            const sortedTypes = Object.keys(issuesByType).sort((a, b) => {
              return severityOrder.indexOf(a) - severityOrder.indexOf(b);
            });

            summary += `## ğŸ’¡ Suggestions\n\n`;
            summary += `| Category | Count | Files |\n`;
            summary += `|----------|-------|-------|\n`;

            for (const type of sortedTypes) {
              const items = issuesByType[type];
              const emoji = EMOJI_MAP[type] || 'âš ï¸';
              const typeLabel = type.toUpperCase().replace('-', ' ');
              const fileRefs = items.map(i => `\`${i.file.split('/').pop()}:${i.line}\``).join(', ');
              summary += `| ${emoji} **${typeLabel}** | ${items.length} | ${fileRefs} |\n`;
            }
          }

          // Include issues that couldn't be posted inline
          if (failedIssues.length > 0) {
            summary += `\n### Additional findings\n\n`;
            for (const item of failedIssues) {
              const emoji = EMOJI_MAP[item.type] || 'âš ï¸';
              summary += `${emoji} **${item.type.toUpperCase()}** in \`${item.file}:${item.line}\`\n`;
              summary += `> ${item.friendlySuggestion}\n\n`;
            }
          }

          // Post or update summary comment
          const { data: issueComments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });

          const botComment = issueComments.find(c => c.body && c.body.includes(SUMMARY_MARKER));
          const body = `${SUMMARY_MARKER}\n\n${summary}`;

          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body
            });
          }

          // Export analysis_markdown for downstream consumers
          const output = summary.replace(/`/g, '\\`');
          core.setOutput('analysis_markdown', summary);
